{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIa-poEHfw5Z"
      },
      "source": [
        "#Assignment 1 (Random Forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGU057DfgTIL"
      },
      "source": [
        "* In this project you are given a dataset of housing housing price prediction. \n",
        "Dataset description is found in the given datasets.\n",
        "\n",
        "* The goal of the project is to predict the price of a house given its attributes. \n",
        "Therefore, the problem is a regression task. \n",
        "\n",
        "* You need to build a random forest that consists of multiple decision trees (for regression) from the given training data set. Then, apply it on the test set and submit your code to generate predictions.\n",
        "You need to build the random forest and decision trees from scratch. (I.e., it is not allowed to use existing machine learning libraries or packages such as sklearn.)\n",
        "\n",
        "* You may use any programming language/environment of your choice, but you are required to submit the complete source code to produce the output\n",
        "If you use anything other than jupyter notebook, submit an executable and run that from the main function of the jupyter notebook so that the prediction generation is automated. We can provide assistance with this.\n",
        "The output (a single file with the predictions for each test instance) must be generated automatically using the approach implemented by you. Submitting predictions/code from any other source (Internet, another student, etc.) is considered cheating and will result in immediate disqualification (i.e., dismissal from the course)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kut1m20f4zR"
      },
      "source": [
        "##Part 1: Preprocessing and dataset analysis (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yANfSgagRJq"
      },
      "source": [
        "* The given dataset is quite complex, it has many attributes, and not all of them are useful! \n",
        "Training on such dataset results in a bad accuracy. And this is exactly the point! \n",
        "\n",
        "* \"Understanding the question is half the answer\". In data mining, understanding the dataset is half the answer! \n",
        "\n",
        "* In part 1 you need to analyze the dataset and make it clean. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4xjsBzYlfjq"
      },
      "source": [
        "###Load the dataset and explore (5 points)\n",
        "\n",
        "* Load the dataset, view the dataset and the shape of it, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AUvifadOf4G-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Viewing the training dataset -----\n",
            "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "\n",
            "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
            "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
            "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
            "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
            "\n",
            "  YrSold  SaleType  SaleCondition  SalePrice  \n",
            "0   2008        WD         Normal     208500  \n",
            "1   2007        WD         Normal     181500  \n",
            "2   2008        WD         Normal     223500  \n",
            "3   2006        WD        Abnorml     140000  \n",
            "4   2008        WD         Normal     250000  \n",
            "\n",
            "[5 rows x 81 columns]\n",
            "\n",
            "----- Viewing the test dataset -----\n",
            "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
            "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
            "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
            "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
            "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
            "\n",
            "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
            "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
            "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
            "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
            "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
            "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
            "\n",
            "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
            "0       0      6    2010        WD         Normal  \n",
            "1   12500      6    2010        WD         Normal  \n",
            "2       0      3    2010        WD         Normal  \n",
            "3       0      6    2010        WD         Normal  \n",
            "4       0      1    2010        WD         Normal  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "\n",
            "=== Missing Values in Training Data ===\n",
            "LotFrontage      259\n",
            "Alley           1369\n",
            "MasVnrType       872\n",
            "MasVnrArea         8\n",
            "BsmtQual          37\n",
            "BsmtCond          37\n",
            "BsmtExposure      38\n",
            "BsmtFinType1      37\n",
            "BsmtFinType2      38\n",
            "Electrical         1\n",
            "FireplaceQu      690\n",
            "GarageType        81\n",
            "GarageYrBlt       81\n",
            "GarageFinish      81\n",
            "GarageQual        81\n",
            "GarageCond        81\n",
            "PoolQC          1453\n",
            "Fence           1179\n",
            "MiscFeature     1406\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training and test datasets. \n",
        "# NA values are given the value of 0 aka null\n",
        "# consider poor as value of 1\n",
        "train_df = pd.read_csv('housing_price_train.csv', na_values='NA')\n",
        "test_df = pd.read_csv('housing_price_test.csv', na_values='NA')\n",
        "\n",
        "# Display basic information about the datasets\n",
        "print(\"----- Viewing the training dataset -----\")\n",
        "print(train_df.head())\n",
        "#Header already shows the shape\n",
        "#print(f\"Shape: {test_df.shape} (rows, columns)\")\n",
        "\n",
        "\n",
        "print(\"\\n----- Viewing the test dataset -----\")\n",
        "print(test_df.head())\n",
        "\n",
        "\n",
        "# Check for missing values (example for training data)\n",
        "print(\"\\n=== Missing Values in Training Data ===\")\n",
        "list_of_null= train_df.isnull().sum()\n",
        "print(list_of_null[list_of_null> 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWmPCu5hmAIj"
      },
      "source": [
        "### Clean the dataset (10 points)\n",
        "\n",
        "* We cannot train on a 'dirty' dataset! There are duplicated, Null, and missing values that you need to take care of!\n",
        "\n",
        "* Drop all columns which have null values >= 70 % and drop all rows which have null values >= 70 %.\n",
        "\n",
        "* You need to take care of categorial data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "exQbzeH7mM-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def PreprocessingData(db):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset:\n",
        "    - Show NULL values for each column and their percentage.\n",
        "    - Drop all columns with null values >= 70%.\n",
        "    - Drop all rows with null values >= 70%.\n",
        "    - Fill missing values with the mean (numerical) or mode (categorical).\n",
        "    \"\"\"\n",
        " # Create a copy to avoid modifying the original dataframe\n",
        "    df_clean = db.copy()\n",
        "\n",
        "    # Display NULL values information\n",
        "    null_counts = df_clean.isnull().sum()\n",
        "    null_percentage = (null_counts / len(df_clean)) * 100\n",
        "    #print(\"\\n=== Missing Values Percentage ===\")\n",
        "    #print(null_percentage[null_percentage > 0].round(2))\n",
        "\n",
        "    # Drop columns with >= 70% null values\n",
        "    df_clean = df_clean.dropna(axis=1, thresh=int(0.3 * len(df_clean)))\n",
        "\n",
        "    # Drop rows with >= 70% null values\n",
        "    df_clean = df_clean.dropna(axis=0, thresh=int(0.3 * df_clean.shape[1]))\n",
        "\n",
        "    # Separate categorical and numerical columns\n",
        "    cat_cols = df_clean.select_dtypes(include=['object']).columns  # FIXED: 'object' instead of 'category'\n",
        "    num_cols = df_clean.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Fill numerical nulls with median\n",
        "    df_clean[num_cols] = df_clean[num_cols].fillna(df_clean[num_cols].median())\n",
        "\n",
        "    # Fill categorical nulls with mode\n",
        "    for col in cat_cols:\n",
        "        df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0] if not df_clean[col].mode().empty else \"missing\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "train_df = pd.read_csv('housing_price_train.csv', na_values='NA')\n",
        "prepro_train_df = PreprocessingData(train_df)\n",
        "#prepro_train_df.to_csv(\"train_preprocessed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ8kz_a-mdNx"
      },
      "source": [
        "\n",
        "### Correlations! (5 points)\n",
        "\n",
        "* Now we have a clean dataset, but not all attributes are useful! \n",
        "\n",
        "* Display the corrlation between all features and the sales price. This will show you which feature affects sales price more. You may use *corr()* function. \n",
        "\n",
        "* Choose the most correlated features, and remove others. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Irh60fo-nVtw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Correlation with SalePrice:\n",
            " SalePrice        1.000000\n",
            "OverallQual      0.790982\n",
            "GrLivArea        0.708624\n",
            "GarageCars       0.640409\n",
            "GarageArea       0.623431\n",
            "TotalBsmtSF      0.613581\n",
            "1stFlrSF         0.605852\n",
            "FullBath         0.560664\n",
            "TotRmsAbvGrd     0.533723\n",
            "YearBuilt        0.522897\n",
            "YearRemodAdd     0.507101\n",
            "MasVnrArea       0.472614\n",
            "Fireplaces       0.466929\n",
            "GarageYrBlt      0.466754\n",
            "BsmtFinSF1       0.386420\n",
            "LotFrontage      0.334771\n",
            "WoodDeckSF       0.324413\n",
            "2ndFlrSF         0.319334\n",
            "OpenPorchSF      0.315856\n",
            "HalfBath         0.284108\n",
            "LotArea          0.263843\n",
            "BsmtFullBath     0.227122\n",
            "BsmtUnfSF        0.214479\n",
            "BedroomAbvGr     0.168213\n",
            "ScreenPorch      0.111447\n",
            "PoolArea         0.092404\n",
            "MoSold           0.046432\n",
            "3SsnPorch        0.044584\n",
            "BsmtFinSF2      -0.011378\n",
            "BsmtHalfBath    -0.016844\n",
            "MiscVal         -0.021190\n",
            "Id              -0.021917\n",
            "LowQualFinSF    -0.025606\n",
            "YrSold          -0.028923\n",
            "OverallCond     -0.077856\n",
            "MSSubClass      -0.084284\n",
            "EnclosedPorch   -0.128578\n",
            "KitchenAbvGr    -0.135907\n",
            "Name: SalePrice, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def corr(df):\n",
        "    df_numeric = df.select_dtypes(include=['number'])\n",
        "    correlation_matrix = df_numeric.corr()\n",
        "    sales_price_corr = correlation_matrix[\"SalePrice\"].sort_values(ascending=False)\n",
        "\n",
        "    print(\"Feature Correlation with SalePrice:\\n\", sales_price_corr)\n",
        "\n",
        "    # Select features with absolute correlation > 0.5 (excluding SalePrice)\n",
        "    strong_features = sales_price_corr[abs(sales_price_corr) > 0.5].index\n",
        "\n",
        "    if len(strong_features) <= 1:\n",
        "        print(\"No strong correlations found. Returning original dataset.\")\n",
        "        return df_numeric  # Return unfiltered numeric data if no strong correlations\n",
        "\n",
        "    df_filtered = df_numeric[strong_features]\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "\n",
        "train_corr = corr(prepro_train_df)\n",
        "#train_corr.to_csv(\"train_corr.csv\", index=False) if not train_corr.empty else print(\"No data to save.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnNAO-wnXM6"
      },
      "source": [
        "## Part 2: Decision Tree (45 points)\n",
        "#### Building a Decision Tree:\n",
        "A Decision tree consists of nodes connected by edges. A decision tree is typically, a binary tree, which has the following properties:\n",
        "- One node is marked as Root node\n",
        "- Every node other than the root has a parent node\n",
        "- Each node can have at most 2 child nodes (left edge & right edge)\n",
        "- Leaf node is the node which contains pure data or when we reach to the maximum depth \n",
        "\n",
        "To create the decision tree model for scratch you need to create two classes (a class for the node, for example \"class DecisionNode():\" and a class for Decision Tree model, for example \"class RegressionDecisionTree():\")\n",
        "\n",
        "\n",
        "1- DecisionNode class used to save some values for each node we do the spliting on it until we reach the leaf node\n",
        "so we will save the following values for the node:\n",
        "- feature: feature index.\n",
        "- threshold: the value we used to split the data on.\n",
        "- value: the average value for the leaf node.\n",
        "- True_Branch: if the condition is true.\n",
        "- False_Branch: if the condition is false."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cYNFB7QY-SJK"
      },
      "outputs": [],
      "source": [
        "class DecisionNode():\n",
        "    def __init__(self, feature_idx=None, threshold=None, value=None, true_branch=None, false_branch=None):\n",
        "        self.feature_idx = feature_idx # index of the feature that is used\n",
        "        self.threshold = threshold     # threshold value for feature when making the decision\n",
        "        self.value = value # Average value if the node is a leaf in the tree\n",
        "        self.true_branch = true_branch # the node we go to if decision returns True\n",
        "        self.false_branch = false_branch # the node we go to if decision returns False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYpCv95F-SJK"
      },
      "source": [
        "# Decision Tree Class\n",
        "This Class consists the following functions:\n",
        "<ol>\n",
        "<li> <b>build_tree</b>: used to create the decision tree nodes</li> \n",
        "<li> <b>calc_variance_reduction</b> : measure the impurity by using variance reduction measure (like MSE) </li> \n",
        "the function takes three parameters (parentRec: the records for the target before split,and the left and right records after splitting. This function used to measure the impurity for each node and decide if we will split or not.\n",
        "<li> <b>majority_vote</b>: used to calculate values for the leaf nodes records which equal to the mean of these records.</li> \n",
        "<li><b>split_by_feature</b>: this function take the feature and the threshold and check if the feature is numerical so it split the records into two node (true which is the left edge and false which is the right edge)\n",
        "if the feature is categorical so it split where the values equal to the threshold</li>\n",
        "<li> <b>fit</b>: Used to train the dataset after spliting the data into two part x: features, y: target</li>\n",
        "<li><b>predict_value</b>: used to predict the value for each record, it is a recursive method to find the leaf node that corresponds to prediction\n",
        "<li><b>predict</b>: take all records for the test data and iterate into each record to predit the y(target) value and save the result into a prediction list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HSe6v4r1-SJK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class RegressionDecisionTree():\n",
        "    # constructor\n",
        "    def __init__(self, min_VarianceReduction=1e-7, max_depth=5):        \n",
        "        self.root = None # root of this tree\n",
        "        self.min_VarianceReduction = min_VarianceReduction # minimum VarianceReduction to allow splitting\n",
        "        # used to stopping conductions\n",
        "        self.max_depth = max_depth # maximum depth the tree grows to\n",
        " \n",
        "\n",
        "    # used to create the decision tree nodes\n",
        "    def build_tree(self, X, y, current_depth=0):\n",
        "        # we will use decision dictionary to save the feature and the threshold we build the tree on \n",
        "        decision = None\n",
        "        # we will use subtrees dictionary to save the feature and the threshold we build the tree on \n",
        "        subtrees = None\n",
        "        largest_variance_Reduction = 0\n",
        "        # add y as last column of X\n",
        "        df = pd.concat((X, y), axis=1)\n",
        "        n_rows, n_features = X.shape\n",
        "        if current_depth <= self.max_depth:\n",
        "            # iterate through every feature\n",
        "            for feature_idx in range(n_features):\n",
        "                # values of that column\n",
        "                feature_values = X.iloc[:, feature_idx]                \n",
        "                unique_values = feature_values.unique()                \n",
        "                for threshold in unique_values:\n",
        "                    X_trueEdge, X_falseEdge = self.split_by_feature(df, feature_idx, threshold)\n",
        "                    if len(X_trueEdge) > 0 and len(X_falseEdge) > 0:\n",
        "                        y_true = X_trueEdge.iloc[:,-1]\n",
        "                        y_false = X_falseEdge.iloc[:,-1]                        \n",
        "                        # Calculate impurity\n",
        "                        VarianceRed = self.Calc_variance_reduction(y, y_true, y_false)\n",
        "                        # Keep track of which feature gave the largest information gain\n",
        "                        if VarianceRed > largest_variance_Reduction:\n",
        "                            largest_variance_Reduction = VarianceRed\n",
        "                            decision = {\"feature_idx\":feature_idx, \"threshold\":threshold}\n",
        "                            subtrees = {\"X_true\":X_trueEdge.iloc[:,:-1],\n",
        "                                        \"y_true\":y_true,\n",
        "                                        \"X_false\":X_falseEdge.iloc[:,:-1],\n",
        "                                        \"y_false\":y_false}\n",
        "\n",
        "        # we will construct new branch of tree if the variance_Reduction is larger than minimum variance_Reduction that we've defined\n",
        "        if largest_variance_Reduction > self.min_VarianceReduction:\n",
        "            true_branch = self.build_tree(subtrees[\"X_true\"], subtrees[\"y_true\"], current_depth+1)\n",
        "            false_branch = self.build_tree(subtrees[\"X_false\"], subtrees[\"y_false\"], current_depth+1)\n",
        "            return DecisionNode(feature_idx=decision[\"feature_idx\"], threshold=decision[\"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
        "\n",
        "        # at leaf node we calculate the mean for the records\n",
        "        leaf_value = self.majority_vote(y)\n",
        "        return DecisionNode(value=leaf_value)\n",
        "                        \n",
        "    # measure the impurity by using variance reduction measure (like MSE)\n",
        "    # left_edgeRec= True edge: where condition is true\n",
        "    # Right_edgeRec= False edge: where condition is false\n",
        "    def Calc_variance_reduction(self, parentRec, left_edgeRec, Right_edgeRec):   \n",
        "        # return the VarReduction = variance for parent - (Weight * var(leftEdge) + Weight * var(RightEdge)\n",
        "        var_parent = np.var(parentRec)\n",
        "        \n",
        "        weight_left = len(left_edgeRec) / len(parentRec)\n",
        "        weight_right = len(Right_edgeRec) / len(parentRec)\n",
        "        var_left = np.var(left_edgeRec)\n",
        "        var_right = np.var(Right_edgeRec)\n",
        "\n",
        "        VarReduction = var_parent - (weight_left * var_left + weight_right * var_right)\n",
        "        return VarReduction\n",
        "        \n",
        "    \n",
        "    def majority_vote(self, Y): \n",
        "        # return the majority_vote for the leaf nodes \n",
        "        return Y.mean()\n",
        "        \n",
        "             \n",
        "    def split_by_feature(self, db, feature_idx, threshold):\n",
        "        # split the data into left_edge & right_edge depends one specified feature and the threshold\n",
        "        left_edge = db[db.iloc [:, feature_idx] <= threshold]\n",
        "        right_edge = db[db.iloc [:, feature_idx] > threshold]\n",
        "        # return left & right edges\n",
        "        return left_edge, right_edge\n",
        "\n",
        "    \n",
        "    # Used to train the dataset after spliting the data into x: features, y: target\n",
        "    def fit(self, X, y):\n",
        "        self.root = self.build_tree(X, y)\n",
        "\n",
        "\n",
        "    def predict_value(self, xTest, tree=None):\n",
        "        # recursive method to find the leaf node that corresponds to prediction\n",
        "        if tree is None:\n",
        "            tree = self.root\n",
        "        \n",
        "        # If it's a leaf node, return the value\n",
        "        if tree.value is not None:\n",
        "            return tree.value\n",
        "        \n",
        "        if xTest[tree.feature_idx] <= tree.threshold:\n",
        "            return self.predict_value(xTest, tree.true_branch)\n",
        "        else:\n",
        "            return self.predict_value(xTest, tree.false_branch) \n",
        "\n",
        "    # to predict the value we need to pass the all records for features and we save the prediction for each records into a list\n",
        "    def predict(self, XTest):\n",
        "        y_pred = []\n",
        "        for idx, row in XTest.iterrows():           \n",
        "            y_pred.append(self.predict_value(row.values))\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9aqjovq-SJM"
      },
      "source": [
        "- To Check the Accuracy for our prediction we use CalcAccuracy function which take the actual values for the test dataset and the predicted values and apply the RMSE formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "24p2WfE8-SJN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def CalcAccuracy(Actual_Y, Predicted_y):\n",
        "    Actual_Y = np.array(Actual_Y)\n",
        "    Predicted_y = np.array(Predicted_y)\n",
        "    \n",
        "    # Calculate the squared differences\n",
        "    squared_diff = (Actual_Y - Predicted_y) ** 2\n",
        "    \n",
        "    # Calculate the mean of the squared differences\n",
        "    mean_squared_diff = np.mean(squared_diff)\n",
        "    \n",
        "    # Take the square root to get RMSE\n",
        "    rmse = np.sqrt(mean_squared_diff)\n",
        "    \n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_2Ve9v--SJN"
      },
      "source": [
        "- Build decision tree model\n",
        "- Fit the model\n",
        "- Predict the values from test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7dJKxolT-SJN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 42621.33219017309\n",
            "Relative RMSE: 23.98%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = pd.read_csv('housing_price_train.csv')\n",
        "test_data = pd.read_csv('housing_price_test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Preprocess the data \n",
        "train_data_clean = PreprocessingData(train_data)\n",
        "test_data_clean = PreprocessingData(test_data)\n",
        "\n",
        "# Select the strong features based on correlation with SalePrice\n",
        "train_data_filtered = corr(train_data_clean)\n",
        "\n",
        "# Separate the features (X) and target (y) for training\n",
        "X_train = train_data_filtered.drop(columns=['SalePrice'])\n",
        "y_train = train_data_filtered['SalePrice']\n",
        "\n",
        "# Prepare the test data (features only, no target)\n",
        "X_test = test_data_clean[X_train.columns]  # Ensure test data has the same features as training data\n",
        "\n",
        "# Initialize and train the RegressionDecisionTree model\n",
        "regression_tree = RegressionDecisionTree()\n",
        "regression_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict the values using the trained model\n",
        "y_pred = regression_tree.predict(X_test)\n",
        "\n",
        "# Merge the predictions with the sample_submission to compare with the actual SalePrice\n",
        "predictions_df = pd.DataFrame({'Id': test_data['Id'], 'Predicted_SalePrice': y_pred})\n",
        "\n",
        "# Merge the predictions with the sample_submission for RMSE calculation\n",
        "merged_predictions = pd.merge(sample_submission, predictions_df, on='Id')\n",
        "\n",
        "# Calculate RMSE (Root Mean Squared Error)\n",
        "rmse = CalcAccuracy(merged_predictions['SalePrice'], merged_predictions['Predicted_SalePrice'])\n",
        "mean_saleprice = np.mean(merged_predictions['SalePrice'])\n",
        "relative_rmse = rmse / mean_saleprice\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Relative RMSE: {relative_rmse * 100:.2f}%\")\n",
        "\n",
        "#predictions_df.to_csv('predictions.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvzaxcPAoJsk"
      },
      "source": [
        "## Part 3: Random Forest (20 points)\n",
        "#### Random forest class\n",
        "- the Class consist of the following functions:\n",
        "<ul>\n",
        "    <li>Constructor: consists of the subset data (Training & Testing) dataset after preprocessing and a list of deciceion tree objects </li>    \n",
        "    <li>Subsampling: Bagging we will take random sample with replacement for the Training dataset </li>\n",
        "    <li>build_model: first make subsample for the training dataset, then split the data into featurespart(X) and targetpart(Y), then take 10 samples of the feature part, finally build the decision tree (fit), this function take the number of DT that we want to build</li>\n",
        "    <li>predict: take the test dataset and make the prediction for the target field in all the tree in the random forest then take the mean for the prediction in each tree, finally add the mean of prediction to a list of predition </li>\n",
        "\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "a00zi12Q-SJO"
      },
      "outputs": [],
      "source": [
        "class RF(object):\n",
        "    def __init__(self):\n",
        "        self.Traindata = None  # training data set (loaded into memory)\n",
        "        self.Testdata = None  # Test data set for prediction        \n",
        "        self.trees = []  # list of decision trees \n",
        "          \n",
        "\n",
        "     # This function generate a subsample with replacement\n",
        "    def __subsampling(self, train_set, sample_size_ratio):       \n",
        "\n",
        "        sample_number = round(len(train_set) * sample_size_ratio)\n",
        "        return train_set.sample(n = sample_number, replace=True)\n",
        "\n",
        "        \n",
        "        \n",
        "    def build_model(self, train_set, sample_size_ratio, number_of_trees):\n",
        "        for i in range(number_of_trees):\n",
        "            TrainingSample = self.__subsampling(train_set, sample_size_ratio)\n",
        "            \n",
        "            # Split the training data into features (X) and target (y)\n",
        "            X_train = TrainingSample.drop(columns=['SalePrice'])  # Assuming 'SalePrice' is the target column\n",
        "            y_train = TrainingSample['SalePrice']\n",
        "            \n",
        "            # build a tree\n",
        "            tree = RegressionDecisionTree() \n",
        "            # Train the tree \n",
        "            tree.fit(X_train, y_train)  \n",
        "            \n",
        "            self.trees.append(tree)\n",
        "            \n",
        "               \n",
        "    def predict(self, test_set):\n",
        "        predictions = []\n",
        "        for tree in self.trees:\n",
        "            # Predict for each item in the list \n",
        "            X_test = test_set.drop(columns=['SalePrice'], errors='ignore')  # SalePrice is not in test data\n",
        "            y_pred_tree = tree.predict(X_test)\n",
        "            predictions.append(y_pred_tree)\n",
        "        \n",
        "        # Calculate the mean \n",
        "        predictions = np.array(predictions)\n",
        "        mean_predictions = np.mean(predictions, axis=0)\n",
        "        \n",
        "        return mean_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITGUmaXc-SJO"
      },
      "source": [
        "### Create Random Forest\n",
        "\n",
        "* Create 10 Decision Tree in the randomforest\n",
        "* Train the random forest with the dataset\n",
        "* Use the created random forest to predict the test dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SjqaGyyl-qoY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 38390.22841121438\n",
            "Relative RMSE: 21.60%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Load the training and test data\n",
        "train_data = pd.read_csv('housing_price_train.csv')\n",
        "test_data = pd.read_csv('housing_price_test.csv')\n",
        "\n",
        "# Preprocess the data \n",
        "train_data_clean = PreprocessingData(train_data)\n",
        "test_data_clean = PreprocessingData(test_data)\n",
        "\n",
        "# Select the strong features based on correlation with SalePrice\n",
        "train_data_filtered = corr(train_data_clean)\n",
        "\n",
        "# Separate the features (X) and target (y) for training\n",
        "X_train = train_data_filtered.drop(columns=['SalePrice'])\n",
        "y_train = train_data_filtered['SalePrice']\n",
        "\n",
        "# Prepare the test data (features only, no target)\n",
        "#X_test = test_data_clean[X_train.columns]  # Ensure test data has the same features as training data\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RF()\n",
        "\n",
        "# Train the random forest with 10 decision trees, using 80% of data for sub sampling\n",
        "rf_model.build_model(train_data_filtered, sample_size_ratio=0.8, number_of_trees=10)\n",
        "\n",
        "# Predict the values using the trained random forest model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Create the predictions DataFrame\n",
        "predictions_df = pd.DataFrame({'Id': test_data['Id'], 'Predicted_SalePrice': y_pred_rf})\n",
        "\n",
        "# Merge predictions with the sample_submission for RMSE calculation\n",
        "merged_predictions = pd.merge(sample_submission, predictions_df, on='Id')\n",
        "rmse = CalcAccuracy(merged_predictions['SalePrice'], merged_predictions['Predicted_SalePrice'])\n",
        "mean_sale_price = np.mean(merged_predictions['SalePrice'])\n",
        "relative_rmse = rmse / mean_sale_price\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Relative RMSE: {relative_rmse * 100:.2f}%\")\n",
        "\n",
        "\n",
        "#predictions_df.to_csv('rf_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVGAlCDkoRfo"
      },
      "source": [
        "##Part4: Comparison! (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHF7qjiBl6A"
      },
      "source": [
        "Now that you have finished implementing your Random Forest, it's time for some experiments and analysis! \n",
        "\n",
        "* Use the Random Forest in the scikit-learn library and train it on the same dataset. \n",
        "\n",
        "* Compare the accuracy given by your Random Forest to the scikit-learn one. \n",
        "\n",
        "* Increase the number of trees in your Random Forest. Does it improve the accuracy? \n",
        "\n",
        "* Make a table for comparing your Random Forest accuracy with different number of trees with the scikit-learn one. What is your conclusion? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sWmfgEVp-SJO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison of RMSE for different tree counts:\n",
            "     Model  Number of Trees          RMSE\n",
            "0       RF                1  44059.336131\n",
            "1  Sklearn                1  37544.002866\n",
            "2       RF                5  38897.349281\n",
            "3  Sklearn                5  37544.002866\n",
            "4       RF               10  38415.890606\n",
            "5  Sklearn               10  37544.002866\n",
            "6       RF               15  38217.491259\n",
            "7  Sklearn               15  37544.002866\n",
            "8       RF               20  38344.172911\n",
            "9  Sklearn               20  37544.002866\n",
            "Difference in RMSE between RF and Sklearn:\n",
            "  Number of Trees  RMSE Difference\n",
            "0              RF      6515.333266\n",
            "1              RF      1353.346415\n",
            "2              RF       871.887740\n",
            "3              RF       673.488394\n",
            "4              RF       800.170045\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor  \n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv('housing_price_train.csv')\n",
        "test_data = pd.read_csv('housing_price_test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv') \n",
        "\n",
        "# Preprocess the data \n",
        "train_data_clean = PreprocessingData(train_data)\n",
        "test_data_clean = PreprocessingData(test_data)\n",
        "\n",
        "# Select strong features\n",
        "train_data_filtered = corr(train_data_clean)\n",
        "\n",
        "# Split into features and target\n",
        "X_train = train_data_filtered.drop(columns=['SalePrice'])\n",
        "y_train = train_data_filtered['SalePrice']\n",
        "X_test = test_data_clean[X_train.columns]  # Ensure test data has the same features\n",
        "\n",
        "# List of different tree counts to test\n",
        "tree_counts = [1, 5, 10, 15, 20]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Run for different tree counts\n",
        "for num_trees in tree_counts:\n",
        "    rf_model = RF()\n",
        "    rf_model.build_model(train_data_filtered, sample_size_ratio=0.8, number_of_trees=num_trees)\n",
        "    y_pred_rf = rf_model.predict(X_test)\n",
        "    y_pred_rf = y_pred_rf[:len(sample_submission)]\n",
        "\n",
        "    # RF\n",
        "    if \"SalePrice\" in sample_submission.columns:\n",
        "        rmse_rf = CalcAccuracy(sample_submission['SalePrice'], y_pred_rf)\n",
        "        results.append((\"RF\", num_trees, rmse_rf))\n",
        "    else:\n",
        "        print(\"Error: 'SalePrice' column missing in sample_submission.\")\n",
        "\n",
        "    # scikit-learn\n",
        "    try:\n",
        "        sk_rf_model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "        sk_rf_model.fit(X_train, y_train)\n",
        "        y_pred_sk_rf = sk_rf_model.predict(X_test)\n",
        "        y_pred_sk_rf = y_pred_sk_rf[:len(sample_submission)]\n",
        "        rmse_sk_rf = CalcAccuracy(sample_submission['SalePrice'], y_pred_sk_rf)\n",
        "        results.append((\"Sklearn\", num_trees, rmse_sk_rf))\n",
        "    except ModuleNotFoundError:\n",
        "        print(\"Error: scikit-learn is not installed.\")\n",
        "\n",
        "# Calculate the difference in RMSE between RF and sklearn models\n",
        "rmse_diff = []\n",
        "for result_rf, result_sk in zip(results[::2], results[1::2]):  # Pair up RF and sklearn results\n",
        "    num_trees_rf, _, rmse_rf_value = result_rf\n",
        "    num_trees_sk, _, rmse_sk_value = result_sk\n",
        "    rmse_diff.append((num_trees_rf, rmse_rf_value - rmse_sk_value))\n",
        "\n",
        "# Display RMSE comparison\n",
        "print(\"Comparison of RMSE for different tree counts:\")\n",
        "print(pd.DataFrame(results, columns=[\"Model\", \"Number of Trees\", \"RMSE\"]))\n",
        "\n",
        "# Display RMSE difference\n",
        "print(\"Difference in RMSE between RF and Sklearn:\")\n",
        "print(pd.DataFrame(rmse_diff, columns=[\"Number of Trees\", \"RMSE Difference\"]))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

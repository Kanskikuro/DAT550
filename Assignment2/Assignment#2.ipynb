{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjcahcys1yO"
      },
      "source": [
        "# Assignment #2\n",
        "\n",
        "# In This assignment you are asked to read a data which include 48505 articles (Documents). Then fint the most similar documents using Locality Sensitive Hashing. Follow the lecture covering this topic step by step.\n",
        "\n",
        "## 1. Data is available in Json format and you need to read it. 'https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json' (5 points)\n",
        "## 2. Shingle the documents (10 points)\n",
        "### Tips:\n",
        "* Use string package to cleanup the articles e.g, str.maketrans('', '', string.\n",
        "punctuation)\n",
        "* It is better to convert text to lower case that way you get fewer n-grams\n",
        "* apply ngrams(x.split(), n) using ngrams from nltk on the content + title for computing n-grams, for this data n = 2 is suffcient\n",
        "  * You can use n-gram at word level for this task\n",
        "  * try with different n-gram values \n",
        "  * You can use ngrams from nltk for this\n",
        "\n",
        "## 3. Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big. (10 points)\n",
        "* For example,\n",
        "\n",
        "  * Select top 10000 most frequent n-grams.\n",
        "  * You may also try smaller values of n (like 2 or 3) which result in fewer n-grams.\n",
        "  * Finally, you can also try sparse matrix representation. Like csr_matrix from scipy.sparse. It works even with full vocabulary.\n",
        "    * Given a list of n-grams for each document, see how to builid a sparse matrix here https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
        "\n",
        "## 4. We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much. (10 points)\n",
        "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
        "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
        "* It can also be a as simple as (rownumber * randint) % Numberofbuckets\n",
        "\n",
        "## 5. Compute minhash following the faster algorithm from the lecture (10 points)\n",
        "## 6. Hash signature bands into buckets. Find a way to combine all the  signature values in a band and hash them into a number of buckets ususally very high. (10 points)\n",
        "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before\n",
        "* You should use the same hash function for all bands. And all documents ending up in same bucket for at least one band are considered as candidate pairs.\n",
        "\n",
        "## 7. Tune parameters to make sure the threshold is appropriate. (10 points)\n",
        "* plot the probability of two similar items falling in same bucket for different threshold values\n",
        "\n",
        "## 8. Choose the best parameters and get nearest neighbors of each articles (20 points)\n",
        "* Jaccard Similarity\n",
        "* convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
        "\n",
        "## 9. Write the nearest neibhors of each document to submissions.csv (comma separated, first column is the current document followed by a list of nearest neighbors) file and get the score (10 points)\n",
        "\n",
        "## 10. Write a report + notebook + submission file in a zip file (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yjK4_fGbHYIA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'article_id': 0, 'Title': 'Tikcro enters into research and license agreement with Yeda', 'Content': 'Tikcro enters into research and license agreement with Yeda\\n\\nNews Medical Wednesday 31st December, 2014\\n\\nTikcro Technologies Ltd. (OTC PK: TIKRF) today announced that it has entered into a research and license agreement with Yeda Research and Development Company Ltd., the technology transfer arm of the Weizmann Institute of Science in Israel. This agreement is for the development of new antibodies originating from specified research at the Weizmann Institute of Science addressing identified targets of cancer immune checkpoints.\\n\\nUnder the agreement, Tikcro will provide funding for further research at the Weizmann Institute of Science to develop certain antibodies selected and verified in pre-clinical trials. The antibodies may have high selectivity and binding qualities towards cancer immune checkpoints. Further research and development will be required to promote such antibodies as therapeutic candidates for immune modulation in oncology.\\n\\nTikcro, alone or through sub-licensees, will have the right to obtain the research results and to pursue development through commercialization. The license consideration due from Tikcro to Yeda includes royalties from net sales, sub-license fees and fixed fees linked to clinical and commercial sales milestones.\\n\\n...\\n\\nRead more'}\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "import requests\n",
        "import json\n",
        "\n",
        "###############################--- Step 1 Reading data ---###############################\n",
        "\n",
        "url = 'https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json'\n",
        "response = requests.get(url)\n",
        "articles = json.loads(response.text)\n",
        "print(articles[0])\n",
        "# Json format: article_id, Title, Content\n",
        "\n",
        "###############################--- Step 2 Shingle ---###############################\n",
        "\n",
        "def clean_text(text): # Remove punctuation and convert to lower-case\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator).lower()\n",
        "\n",
        "def get_document_ngrams(article, n=2): # Combine title and content (adjust keys as needed)\n",
        "    combined_text = article.get('title', '') + \" \" + article.get('content', '')\n",
        "    cleaned = clean_text(combined_text)\n",
        "    return set(ngrams(cleaned.split(), n))\n",
        "\n",
        "documents_ngrams = [get_document_ngrams(article, n=2) for article in articles]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "coy5Lxd6JcaI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set()\n"
          ]
        }
      ],
      "source": [
        "print(documents_ngrams)\n",
        "\n",
        "def getFrequentNgrams(articles):\n",
        "  # Your code Here\n",
        "  # Select most frequent n-grams\n",
        "  return  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaAvfYIkJvYf"
      },
      "outputs": [],
      "source": [
        "def getBinaryMatrix(docs):\n",
        "  # Your code Here\n",
        "       # return binary_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX7R93sYMoM0"
      },
      "outputs": [],
      "source": [
        "def getHashFunctionValues(numrows, numhashfunctions):\n",
        "    # Your code Here\n",
        "    #return a matrix with hash values\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trWgSifwNEQ2"
      },
      "outputs": [],
      "source": [
        "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix):\n",
        "    #return minhash signature matrix\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRC-FWUSSAON"
      },
      "outputs": [],
      "source": [
        "def getLSH(signature_matrix, num_bands, num_buckets):\n",
        "    #return lsh buckets or hash table\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNmDBSVuSdvV"
      },
      "outputs": [],
      "source": [
        "def plotProbability(s, b, r):\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYqQg7BfVPuN"
      },
      "outputs": [],
      "source": [
        "def getJaccardSimilarityScore(C1, C2):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qzcoHoGVasE"
      },
      "outputs": [],
      "source": [
        "# convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
        "nearest_neighbors = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkEBs1OOWbuZ"
      },
      "outputs": [],
      "source": [
        "# Remove the neighbors in same buckets but have similarity score < threshold s\n",
        "n_copy = copy.deepcopy(nearest_neighbors)\n",
        "submission_id = []\n",
        "submission_nid = []\n",
        "for article_id, neighbor_ids in n_copy.items():\n",
        "    for nid in neighbor_ids:\n",
        "        score = \n",
        "        if score < s:\n",
        "           \n",
        "        else:\n",
        "            # add to submission result\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wivKramXf-1"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame()\n",
        "data['article_id'] = submission_id\n",
        "data['neighbor_id'] = submission_nid\n",
        "data.sort_values(by=['article_id', 'neighbor_id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpqOKfR5Xlqz"
      },
      "outputs": [],
      "source": [
        "data.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzmz-qQxXpA6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
